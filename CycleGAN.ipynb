{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\", alt_relu_impl=False):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        if alt_relu_impl:\n",
    "            f1 = 0.5 * (1 + leak)\n",
    "            f2 = 0.5 * (1 - leak)\n",
    "            # lrelu = 1/2 * (1 + leak) * x + 1/2 * (1 - leak) * |x|\n",
    "            return f1 * x + f2 * abs(x)\n",
    "        else:\n",
    "            return tf.maximum(x, leak*x)\n",
    "\n",
    "def instance_norm(x):\n",
    "\n",
    "    with tf.variable_scope(\"instance_norm\"):\n",
    "        epsilon = 1e-5\n",
    "        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n",
    "        scale = tf.get_variable('scale',[x.get_shape()[-1]], \n",
    "            initializer=tf.truncated_normal_initializer(mean=1.0, stddev=0.02))\n",
    "        offset = tf.get_variable('offset',[x.get_shape()[-1]],initializer=tf.constant_initializer(0.0))\n",
    "        out = scale*tf.div(x-mean, tf.sqrt(var+epsilon)) + offset\n",
    "\n",
    "        return out\n",
    "\n",
    "def general_conv2d(inputconv, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=\"VALID\", name=\"conv2d\", do_norm=True, do_relu=True, relufactor=0):\n",
    "    with tf.variable_scope(name):\n",
    "        conv = tf.contrib.layers.conv2d(inputconv, o_d, f_w, s_w, padding, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n",
    "        if do_norm:\n",
    "            conv = instance_norm(conv)\n",
    "            # conv = tf.contrib.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=\"batch_norm\")\n",
    "            \n",
    "        if do_relu:\n",
    "            if(relufactor == 0):\n",
    "                conv = tf.nn.relu(conv,\"relu\")\n",
    "            else:\n",
    "                conv = lrelu(conv, relufactor, \"lrelu\")\n",
    "\n",
    "        return conv\n",
    "\n",
    "\n",
    "def general_deconv2d(inputconv, outshape, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=\"VALID\", name=\"deconv2d\", do_norm=True, do_relu=True, relufactor=0):\n",
    "    with tf.variable_scope(name):\n",
    "        conv = tf.contrib.layers.conv2d_transpose(inputconv, o_d, [f_h, f_w], [s_h, s_w], padding, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n",
    "        if do_norm:\n",
    "            conv = instance_norm(conv)\n",
    "            # conv = tf.contrib.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=\"batch_norm\")\n",
    "            \n",
    "        if do_relu:\n",
    "            if(relufactor == 0):\n",
    "                conv = tf.nn.relu(conv,\"relu\")\n",
    "            else:\n",
    "                conv = lrelu(conv, relufactor, \"lrelu\")\n",
    "\n",
    "        return conv\n",
    "    \n",
    "def build_resnet_block(inputres, dim, name=\"resnet\"):\n",
    "    with tf.variable_scope(name):\n",
    "        out_res = tf.pad(inputres, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
    "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c1\")\n",
    "        out_res = tf.pad(out_res, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
    "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c2\",do_relu=False)\n",
    "        return tf.nn.relu(out_res + inputres)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(inputgen, name=\"generator\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f = 7\n",
    "        ks = 3\n",
    "        ngf = 64\n",
    "\n",
    "        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
    "        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=\"c1\")\n",
    "        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c2\")\n",
    "        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,\"SAME\",\"c3\")\n",
    "\n",
    "        o_r1 = build_resnet_block(o_c3, ngf*4, \"r1\")\n",
    "        o_r2 = build_resnet_block(o_r1, ngf*4, \"r2\")\n",
    "        o_r3 = build_resnet_block(o_r2, ngf*4, \"r3\")\n",
    "        o_r4 = build_resnet_block(o_r3, ngf*4, \"r4\")\n",
    "        o_r5 = build_resnet_block(o_r4, ngf*4, \"r5\")\n",
    "        o_r6 = build_resnet_block(o_r5, ngf*4, \"r6\")\n",
    "\n",
    "        o_c4 = general_deconv2d(o_r6, [1,64,64,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c4\")\n",
    "        o_c5 = general_deconv2d(o_c4, [1,128,128,ngf], ngf, ks, ks, 2, 2, 0.02,\"SAME\",\"c5\")\n",
    "        o_c5_pad = tf.pad(o_c5,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
    "        o_c6 = general_conv2d(o_c5_pad, 3, f, f, 1, 1, 0.02,\"VALID\",\"c6\",do_relu=False)\n",
    "\n",
    "        # Adding the tanh layer\n",
    "\n",
    "        out_gen = tf.nn.tanh(o_c6,\"t1\")\n",
    "\n",
    "\n",
    "        return out_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(inputdisc, name=\"discriminator\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f = 4\n",
    "        ndf = 64\n",
    "\n",
    "        o_c1 = general_conv2d(inputdisc, ndf, f, f, 2, 2, 0.02, \"SAME\", \"c1\", do_norm=False, relufactor=0.2)\n",
    "        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, \"SAME\", \"c2\", relufactor=0.2)\n",
    "        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, \"SAME\", \"c3\", relufactor=0.2)\n",
    "        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 1, 1, 0.02, \"SAME\", \"c4\",relufactor=0.2)\n",
    "        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, \"SAME\", \"c5\",do_norm=False,do_relu=False)\n",
    "\n",
    "        return o_c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_image_pool(num_gens, genimg, gen_pool):\n",
    "    ''' This function saves the generated image to corresponding pool of images.\n",
    "    In starting. It keeps on feeling the pool till it is full and then randomly selects an\n",
    "    already stored image and replace it with new one.'''\n",
    "    pool_size = 50\n",
    "    if(num_gens < pool_size):\n",
    "        gen_pool[num_gens] = genimg\n",
    "        return genimg\n",
    "    else :\n",
    "        p = random.random()\n",
    "        if p > 0.5:\n",
    "            random_id = random.randint(0,pool_size-1)\n",
    "            temp = gen_pool[random_id]\n",
    "            gen_pool[random_id] = genimg\n",
    "            return temp\n",
    "        else :\n",
    "            return genimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "train_a_files = tf.train.match_filenames_once(\"/Users/zjucx/Documents/Github/GAN/dataset/input/monet2photo/trainA/*.jpg\")    \n",
    "train_b_files = tf.train.match_filenames_once(\"/Users/zjucx/Documents/Github/GAN/dataset/input/monet2photo/trainB/*.jpg\")    \n",
    "\n",
    "\n",
    "train_a_queue = tf.train.string_input_producer(train_a_files)\n",
    "train_b_queue = tf.train.string_input_producer(train_b_files)\n",
    "\n",
    "\n",
    "image_reader = tf.WholeFileReader()\n",
    "_, image_a = image_reader.read(train_a_queue)\n",
    "_, image_b = image_reader.read(train_b_queue)\n",
    "\n",
    "image_A = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_a),[256,256]),127.5),1)\n",
    "image_B = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_b),[256,256]),127.5),1)\n",
    "\n",
    "\n",
    "input_A = tf.placeholder(tf.float32, [1, 256, 256, 3], name=\"input_A\")\n",
    "input_B = tf.placeholder(tf.float32, [1, 256, 256, 3], name=\"input_B\")\n",
    "\n",
    "gen_A_pool = tf.placeholder(tf.float32, [None, 256, 256, 3], name=\"gen_A_pool\")\n",
    "gen_B_pool = tf.placeholder(tf.float32, [None, 256, 256, 3], name=\"gen_B_pool\")\n",
    "\n",
    "with tf.variable_scope(\"Model\") as scope:\n",
    "    gena = generator(input_B, name=\"g_B\")\n",
    "    genb = generator(input_A, name=\"g_A\")\n",
    "    dica = discriminator(input_A, name=\"d_A\")\n",
    "    dicb = discriminator(input_B, name=\"d_B\")\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    cyca = generator(genb, name=\"g_B\")\n",
    "    cycb = generator(gena, name=\"g_A\")\n",
    "    dic_gana = discriminator(gena, name=\"d_A\")\n",
    "    dic_ganb = discriminator(genb, name=\"d_B\")\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "\n",
    "    dic_gen_A_pool = discriminator(gen_A_pool, \"d_A\")\n",
    "    dic_gen_B_pool = discriminator(gen_B_pool, \"d_B\")\n",
    "\n",
    "d_loss_a = (tf.reduce_mean(tf.squared_difference(dica, 1)) + tf.reduce_mean(tf.square(dic_gen_A_pool)))/2\n",
    "d_loss_b = (tf.reduce_mean(tf.squared_difference(dicb, 1)) + tf.reduce_mean(tf.square(dic_gen_B_pool)))/2\n",
    "\n",
    "cyc_loss = tf.reduce_mean(tf.abs(input_A-cyca)) + tf.reduce_mean(tf.abs(input_B-cycb))\n",
    "\n",
    "g_loss_a = cyc_loss*10 + tf.reduce_mean(tf.squared_difference(dic_ganb, 1))\n",
    "g_loss_b = cyc_loss*10 + tf.reduce_mean(tf.squared_difference(dic_gana, 1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.0002, beta1=0.5)\n",
    "\n",
    "model_vars = tf.trainable_variables()\n",
    "d_A_vars = [var for var in model_vars if 'd_A' in var.name]\n",
    "g_A_vars = [var for var in model_vars if 'g_A' in var.name]\n",
    "d_B_vars = [var for var in model_vars if 'd_B' in var.name]\n",
    "g_B_vars = [var for var in model_vars if 'g_B' in var.name]\n",
    "\n",
    "d_A_trainer = optimizer.minimize(d_loss_a, var_list=d_A_vars)\n",
    "d_B_trainer = optimizer.minimize(d_loss_b, var_list=d_B_vars)\n",
    "g_A_trainer = optimizer.minimize(g_loss_a, var_list=g_A_vars)\n",
    "g_B_trainer = optimizer.minimize(g_loss_b, var_list=g_B_vars)\n",
    "\n",
    "#for var in model_vars: print(var.name)\n",
    "\n",
    "with tf.control_dependencies([g_A_trainer, d_B_trainer, g_B_trainer, d_A_trainer]):\n",
    "      optimizers = tf.no_op(name='optimizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Size:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e055493330eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                       feed_dict={input_A:A_input[idx], input_B:B_input[idx],\n\u001b[1;32m     34\u001b[0m                                  \u001b[0mgen_A_pool\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgen_image_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gen_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimggena\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgena_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                  gen_B_pool: gen_image_pool(num_gen_inputs, imggenb, genb_pool)}\n\u001b[0m\u001b[1;32m     36\u001b[0m                   )\n\u001b[1;32m     37\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gena_pool = np.zeros((50, 1, 256, 256, 3))\n",
    "genb_pool = np.zeros((50, 1, 256, 256, 3))\n",
    "\n",
    "A_input = np.zeros((100, 1, 256, 256, 3))\n",
    "B_input = np.zeros((100, 1, 256, 256, 3))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "    print(tf.size(train_a_files))\n",
    "    # Loading images into the tensors\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    for idx in range(0, 100):\n",
    "        imga = sess.run(image_A).reshape((1, 256, 256, 3))\n",
    "        imgb = sess.run(image_B).reshape((1, 256, 256, 3))\n",
    "        A_input[idx] = imga\n",
    "        B_input[idx] = imgb\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    num_gen_inputs = 0\n",
    "    \n",
    "    for epoch in range(0,101):\n",
    "        \n",
    "\n",
    "        for idx in range(0, 100):\n",
    "            imggenb, imggena = sess.run([genb, gena],feed_dict={input_A:A_input[idx], input_B:B_input[idx]})\n",
    "            \n",
    "            # train\n",
    "            _, a, b, c, d = (\n",
    "                  sess.run(\n",
    "                      [optimizers, g_loss_a, d_loss_b, g_loss_b, d_loss_a],\n",
    "                      feed_dict={input_A:A_input[idx], input_B:B_input[idx],\n",
    "                                 gen_A_pool: gen_image_pool(num_gen_inputs, imggena, gena_pool),\n",
    "                                 gen_B_pool: gen_image_pool(num_gen_inputs, imggenb, genb_pool)}\n",
    "                  )\n",
    "            )\n",
    "            num_gen_inputs += 1\n",
    "            # Optimizing the G_A network\n",
    "            #_, a, imggena, imggenb = sess.run([g_A_trainer, g_loss_a, genb, gena],feed_dict={input_A:A_input[idx], input_B:B_input[idx]})\n",
    "            #_, b = sess.run([d_B_trainer, d_loss_b],feed_dict={input_A:A_input[idx], input_B:B_input[idx]})\n",
    "            #_, c = sess.run([g_B_trainer, g_loss_b],feed_dict={input_A:A_input[idx], input_B:B_input[idx]})\n",
    "            #_, d = sess.run([d_A_trainer, d_loss_a],feed_dict={input_A:A_input[idx], input_B:B_input[idx]})\n",
    "\n",
    "            print(\"epoch:%d idx:%d g_A_trainer:%f d_B_trainer:%f g_B_trainer:%f d_A_trainer%f\"%(epoch, idx, a, b, c, d))\n",
    "            plt.subplot(141); plt.imshow(((A_input[idx].reshape((256, 256, 3))+1)*127.5).astype(np.uint8))\n",
    "            plt.subplot(142); plt.imshow(((B_input[idx].reshape((256, 256, 3))+1)*127.5).astype(np.uint8))\n",
    "            plt.subplot(143); plt.imshow(((imggena.reshape((256, 256, 3))+1)*127.5).astype(np.uint8))\n",
    "            plt.subplot(144); plt.imshow(((imggenb.reshape((256, 256, 3))+1)*127.5).astype(np.uint8))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
